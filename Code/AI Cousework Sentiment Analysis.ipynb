{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set\n",
    "from keras.datasets import imdb\n",
    "# split the data set into training and testing target-data\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the words already tokenized(remember that the words are unique, each having an ID) -> returns a dictionary\n",
    "index = imdb.get_word_index()\n",
    "# reverse key-value pair in dictionary\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 20 unigrams in the data set\n",
    "# Steps:\n",
    "# 1 decode all the sentences\n",
    "# 2 tokenize\n",
    "# 3 count how many times each word recurs\n",
    "# 4 order the result and print the words\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# STEP 1: decode all the sentences\n",
    "# extract all the sentences from training data and test data decoded\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "decoded_sentences = []\n",
    "# extract from training data\n",
    "for x in range(len(data)):\n",
    "    decoded_sentences.append(\" \".join([reverse_index.get(i - 3, \"\") for i in data[x]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(decoded_sentences)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(decoded_sentences[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentencessentencessentences_sentences = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# The function will split everything in Unigrams\n",
    "def extract_unigram(sentences): \n",
    "\n",
    "  tokens = []\n",
    "  for sentence in sentences:\n",
    "    tok = word_tokenize(sentence)\n",
    "    for t in tok:\n",
    "      tokens.append(t)\n",
    "  return tokens\n",
    "\n",
    "\n",
    "\n",
    "# The function will split everything in Bigrams\n",
    "def extract_bigrams(sentences): \n",
    "\n",
    "  all_bigrams = []\n",
    "  for sentence in sentences:\n",
    "    token = word_tokenize(sentence)\n",
    "    bigrams = ngrams(token,2)\n",
    "    for b in bigrams:\n",
    "      all_bigrams.append(b)\n",
    "  return all_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top unigrams\n",
    "extracted_unigrams = extract_unigram(decoded_sentences);\n",
    "\n",
    "# Get most frequent unigrams into a dictionary\n",
    "frequent_unigrams = dict()\n",
    "for x in extracted_unigrams:\n",
    "    if x in frequent_unigrams:\n",
    "            frequent_unigrams[x] += 1\n",
    "    else:\n",
    "        frequent_unigrams[x] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "is\n",
      "br\n",
      "it\n",
      "in\n",
      "i\n",
      "this\n",
      "that\n",
      "'s\n",
      "was\n",
      "as\n",
      "movie\n",
      "for\n",
      "with\n",
      "but\n",
      "film\n",
      "you\n",
      "on\n",
      "n't\n",
      "not\n",
      "are\n",
      "he\n",
      "his\n",
      "have\n",
      "one\n",
      "be\n",
      "all\n",
      "at\n",
      "they\n",
      "by\n",
      "an\n",
      "who\n",
      "so\n",
      "from\n",
      "like\n",
      "there\n",
      "or\n",
      "just\n",
      "do\n",
      "her\n",
      "out\n",
      "about\n",
      "if\n",
      "has\n",
      "what\n",
      "some\n",
      "good\n",
      "when\n",
      "more\n",
      "very\n",
      "she\n",
      "would\n",
      "up\n",
      "no\n",
      "time\n",
      "my\n",
      "even\n",
      "can\n",
      "which\n",
      "only\n",
      "story\n",
      "really\n",
      "see\n",
      "their\n",
      "had\n",
      "were\n",
      "me\n",
      "did\n",
      "well\n",
      "we\n",
      "does\n",
      "than\n",
      "much\n",
      "could\n",
      "bad\n",
      "get\n",
      "been\n",
      "other\n",
      "people\n",
      "great\n",
      "will\n",
      "also\n",
      "into\n",
      "because\n",
      "how\n",
      "him\n",
      "first\n",
      "most\n",
      "'\n",
      "made\n",
      "its\n",
      "them\n",
      "then\n",
      "make\n",
      "way\n",
      "too\n",
      "movies\n",
      "any\n",
      "after\n",
      "characters\n",
      "think\n",
      "watch\n",
      "character\n",
      "films\n",
      "two\n",
      "many\n",
      "seen\n",
      "being\n",
      "never\n",
      "plot\n",
      "life\n",
      "acting\n",
      "where\n",
      "show\n",
      "best\n",
      "know\n",
      "little\n",
      "over\n",
      "off\n",
      "ever\n",
      "man\n",
      "your\n",
      "better\n",
      "end\n",
      "here\n",
      "scene\n",
      "still\n",
      "say\n",
      "these\n",
      "scenes\n",
      "why\n",
      "should\n",
      "while\n",
      "something\n",
      "'ve\n",
      "such\n",
      "go\n",
      "through\n",
      "back\n",
      "those\n",
      "'m\n",
      "real\n",
      "now\n",
      "thing\n",
      "watching\n",
      "actors\n",
      "director\n",
      "years\n",
      "though\n",
      "old\n",
      "10\n",
      "another\n",
      "work\n",
      "before\n",
      "actually\n",
      "nothing\n",
      "makes\n",
      "look\n",
      "find\n",
      "going\n",
      "same\n",
      "lot\n",
      "new\n",
      "every\n",
      "few\n",
      "again\n",
      "part\n",
      "world\n",
      "'re\n",
      "down\n",
      "cast\n",
      "us\n",
      "things\n",
      "want\n",
      "quite\n",
      "pretty\n",
      "got\n",
      "horror\n",
      "around\n",
      "ca\n",
      "seems\n",
      "young\n",
      "take\n",
      "however\n",
      "thought\n",
      "big\n",
      "fact\n",
      "enough\n",
      "long\n",
      "both\n",
      "give\n",
      "may\n",
      "own\n",
      "between\n",
      "series\n",
      "comedy\n",
      "must\n",
      "right\n",
      "action\n",
      "music\n",
      "without\n",
      "guy\n",
      "times\n",
      "saw\n",
      "always\n",
      "original\n",
      "role\n",
      "come\n",
      "almost\n",
      "gets\n",
      "point\n",
      "interesting\n",
      "done\n",
      "whole\n",
      "least\n",
      "far\n",
      "bit\n",
      "script\n",
      "minutes\n",
      "feel\n",
      "2\n",
      "might\n",
      "anything\n",
      "making\n",
      "since\n",
      "am\n",
      "'ll\n",
      "tv\n",
      "last\n",
      "probably\n",
      "performance\n",
      "kind\n",
      "away\n",
      "girl\n",
      "yet\n",
      "anyone\n",
      "fun\n",
      "woman\n",
      "worst\n",
      "sure\n",
      "rather\n",
      "hard\n",
      "day\n",
      "each\n",
      "played\n",
      "found\n",
      "looking\n",
      "screen\n",
      "although\n",
      "our\n",
      "especially\n",
      "believe\n",
      "dvd\n",
      "having\n",
      "trying\n",
      "course\n",
      "everything\n",
      "set\n",
      "goes\n",
      "ending\n",
      "comes\n",
      "book\n",
      "put\n",
      "maybe\n",
      "place\n",
      "shows\n",
      "three\n",
      "let\n",
      "worth\n",
      "different\n",
      "actor\n",
      "main\n",
      "once\n",
      "someone\n",
      "sense\n",
      "american\n",
      "reason\n",
      "effects\n",
      "play\n",
      "looks\n",
      "money\n",
      "watched\n",
      "true\n",
      "year\n",
      "'d\n",
      "job\n",
      "together\n",
      "war\n",
      "plays\n",
      "high\n",
      "instead\n",
      "during\n",
      "said\n",
      "half\n",
      "later\n",
      "1\n",
      "takes\n",
      "john\n",
      "seem\n",
      "special\n",
      "beautiful\n",
      "night\n",
      "left\n",
      "himself\n",
      "seeing\n",
      "black\n",
      "wife\n",
      "version\n",
      "shot\n",
      "excellent\n",
      "idea\n",
      "house\n",
      "else\n",
      "star\n",
      "mind\n",
      "death\n",
      "fan\n",
      "father\n",
      "used\n",
      "simply\n",
      "nice\n",
      "budget\n",
      "short\n",
      "completely\n",
      "3\n",
      "second\n",
      "men\n",
      "read\n",
      "less\n",
      "along\n",
      "top\n",
      "home\n",
      "help\n",
      "dead\n",
      "line\n",
      "either\n",
      "hollywood\n",
      "kids\n",
      "boring\n",
      "friends\n",
      "camera\n",
      "production\n",
      "try\n",
      "need\n",
      "enjoy\n",
      "wrong\n",
      "use\n",
      "given\n",
      "low\n",
      "classic\n",
      "women\n",
      "full\n",
      "school\n",
      "stupid\n",
      "until\n",
      "next\n",
      "performances\n",
      "rest\n",
      "truly\n",
      "couple\n",
      "video\n",
      "awful\n",
      "start\n",
      "sex\n",
      "recommend\n",
      "tell\n",
      "terrible\n",
      "mean\n",
      "getting\n",
      "came\n",
      "understand\n",
      "perhaps\n",
      "name\n",
      "moments\n",
      "face\n",
      "person\n",
      "keep\n",
      "itself\n",
      "wonderful\n",
      "human\n",
      "mother\n",
      "playing\n",
      "style\n",
      "episode\n",
      "small\n",
      "others\n",
      "boy\n",
      "perfect\n",
      "early\n",
      "stars\n",
      "doing\n",
      "often\n",
      "head\n",
      "definitely\n",
      "written\n",
      "lines\n",
      "children\n",
      "dialogue\n",
      "gives\n",
      "piece\n",
      "went\n",
      "case\n",
      "finally\n",
      "title\n",
      "absolutely\n",
      "live\n",
      "yes\n",
      "laugh\n",
      "certainly\n",
      "friend\n",
      "liked\n",
      "oh\n",
      "become\n",
      "worse\n",
      "entertaining\n",
      "sort\n",
      "cinema\n",
      "lost\n",
      "hope\n",
      "called\n",
      "felt\n",
      "overall\n",
      "guys\n",
      "mr\n",
      "several\n",
      "based\n",
      "supposed\n",
      "5\n",
      "sound\n",
      "drama\n",
      "problem\n",
      "white\n",
      "against\n",
      "waste\n",
      "4\n",
      "beginning\n",
      "fans\n",
      "dark\n",
      "totally\n",
      "care\n",
      "direction\n",
      "humor\n",
      "wanted\n",
      "seemed\n",
      "under\n",
      "lives\n",
      "lead\n",
      "guess\n",
      "example\n",
      "already\n",
      "final\n",
      "throughout\n",
      "becomes\n",
      "unfortunately\n",
      "able\n",
      "quality\n",
      "son\n",
      "history\n",
      "days\n",
      "b\n",
      "side\n",
      "heart\n",
      "fine\n",
      "michael\n",
      "flick\n",
      "wants\n",
      "horrible\n",
      "writing\n",
      "amazing\n",
      "run\n",
      "town\n",
      "\n",
      "act\n",
      "close\n",
      "art\n",
      "child\n",
      "kill\n",
      "god\n",
      "matter\n",
      "etc\n",
      "wo\n",
      "viewer\n",
      "past\n",
      "genre\n",
      "enjoyed\n",
      "brilliant\n",
      "gave\n",
      "behind\n",
      "car\n",
      "eyes\n",
      "parts\n",
      "favorite\n",
      "girls\n",
      "hand\n",
      "kid\n",
      "late\n",
      "city\n",
      "expect\n",
      "hour\n",
      "soon\n",
      "obviously\n",
      "actress\n",
      "themselves\n",
      "sometimes\n",
      "killed\n",
      "thinking\n",
      "starts\n",
      "decent\n",
      "type\n",
      "daughter\n",
      "highly\n",
      "stop\n",
      "self\n",
      "group\n",
      "says\n",
      "voice\n",
      "anyway\n",
      "blood\n",
      "writer\n",
      "took\n",
      "known\n",
      "heard\n",
      "s\n",
      "except\n",
      "fight\n",
      "feeling\n",
      "experience\n",
      "coming\n",
      "slow\n",
      "moment\n",
      "stories\n",
      "leave\n",
      "told\n",
      "extremely\n",
      "score\n",
      "violence\n",
      "police\n",
      "involved\n",
      "strong\n",
      "chance\n",
      "lack\n",
      "hit\n",
      "hero\n",
      "roles\n",
      "hilarious\n",
      "ok\n",
      "particularly\n",
      "living\n",
      "including\n",
      "hell\n",
      "crap\n",
      "brother\n",
      "david\n",
      "please\n",
      "cool\n",
      "cut\n",
      "song\n",
      "complete\n",
      "gore\n",
      "age\n",
      "james\n",
      "serious\n",
      "attempt\n",
      "ago\n",
      "shown\n",
      "taken\n",
      "robert\n",
      "husband\n",
      "english\n",
      "seriously\n",
      "released\n",
      "reality\n",
      "interest\n",
      "jokes\n",
      "opening\n",
      "across\n",
      "none\n",
      "alone\n",
      "exactly\n",
      "possible\n",
      "sad\n",
      "career\n",
      "number\n",
      "saying\n",
      "hours\n",
      "cinematography\n",
      "talent\n",
      "view\n",
      "yourself\n",
      "annoying\n",
      "relationship\n",
      "running\n",
      "order\n",
      "huge\n",
      "shots\n",
      "whose\n",
      "body\n",
      "light\n",
      "ridiculous\n",
      "taking\n",
      "country\n",
      "important\n",
      "power\n",
      "female\n",
      "call\n",
      "started\n",
      "four\n",
      "word\n",
      "turned\n",
      "opinion\n",
      "jack\n",
      "change\n",
      "mostly\n",
      "usual\n",
      "ones\n",
      "room\n",
      "rating\n",
      "novel\n",
      "happy\n",
      "knows\n",
      "knew\n",
      "disappointed\n",
      "apparently\n",
      "non\n",
      "strange\n",
      "attention\n",
      "basically\n",
      "television\n",
      "cheap\n",
      "due\n",
      "musical\n",
      "problems\n",
      "earth\n",
      "miss\n",
      "episodes\n",
      "clearly\n",
      "local\n",
      "7\n",
      "thriller\n",
      "british\n",
      "talk\n",
      "king\n",
      "five\n",
      "events\n",
      "team\n",
      "french\n",
      "moving\n",
      "ten\n",
      "review\n",
      "fast\n",
      "tells\n",
      "8\n",
      "entertainment\n",
      "comic\n",
      "songs\n",
      "straight\n",
      "whether\n",
      "future\n",
      "space\n",
      "dialog\n",
      "above\n",
      "sets\n",
      "easily\n",
      "enjoyable\n",
      "appears\n",
      "near\n",
      "soundtrack\n",
      "hate\n",
      "bring\n",
      "giving\n",
      "romantic\n",
      "similar\n",
      "supporting\n",
      "release\n",
      "mention\n",
      "message\n",
      "filmed\n",
      "sequel\n",
      "eye\n",
      "falls\n",
      "clear\n",
      "needs\n",
      "sister\n",
      "bunch\n",
      "monster\n",
      "surprised\n",
      "rock\n",
      "showing\n",
      "tried\n",
      "sorry\n",
      "certain\n",
      "working\n",
      "easy\n",
      "ways\n",
      "theater\n",
      "theme\n",
      "parents\n",
      "t\n",
      "named\n",
      "storyline\n",
      "stay\n",
      "effort\n",
      "minute\n",
      "lady\n",
      "fall\n",
      "9\n",
      "using\n",
      "tale\n",
      "comments\n",
      "typical\n",
      "editing\n",
      "avoid\n",
      "subject\n",
      "deal\n",
      "doubt\n",
      "richard\n",
      "peter\n",
      "fantastic\n",
      "nearly\n",
      "feels\n",
      "viewing\n",
      "elements\n",
      "general\n",
      "realistic\n",
      "means\n",
      "famous\n",
      "imagine\n",
      "rent\n",
      "crime\n",
      "paul\n",
      "form\n",
      "actual\n",
      "follow\n",
      "red\n",
      "period\n",
      "believable\n",
      "move\n",
      "brought\n",
      "tom\n",
      "forget\n",
      "somehow\n",
      "20\n",
      "begins\n",
      "animation\n",
      "leads\n",
      "imdb\n",
      "figure\n",
      "america\n",
      "weak\n",
      "doctor\n",
      "surprise\n",
      "hear\n",
      "sit\n",
      "average\n",
      "york\n",
      "open\n",
      "fi\n",
      "sequences\n",
      "atmosphere\n",
      "killing\n",
      "eventually\n",
      "learn\n",
      "premise\n",
      "deep\n",
      "wait\n",
      "sci\n",
      "whatever\n",
      "expected\n",
      "dance\n",
      "indeed\n",
      "lame\n",
      "note\n",
      "boys\n",
      "box\n",
      "situation\n",
      "truth\n",
      "third\n",
      "society\n",
      "decided\n",
      "free\n",
      "difficult\n",
      "lee\n",
      "needed\n",
      "romance\n",
      "acted\n",
      "crew\n",
      "leaves\n",
      "unless\n",
      "possibly\n",
      "gay\n",
      "sexual\n",
      "footage\n",
      "credits\n",
      "write\n",
      "forced\n",
      "became\n",
      "begin\n",
      "otherwise\n",
      "air\n",
      "question\n",
      "male\n",
      "meet\n",
      "street\n",
      "meets\n",
      "nature\n",
      "cheesy\n",
      "hands\n",
      "beauty\n",
      "screenplay\n",
      "superb\n",
      "features\n",
      "interested\n",
      "island\n",
      "effect\n",
      "stage\n",
      "comment\n",
      "forward\n",
      "nor\n",
      "badly\n",
      "weird\n",
      "japanese\n",
      "personal\n",
      "quickly\n",
      "total\n",
      "mark\n",
      "keeps\n",
      "towards\n",
      "crazy\n",
      "girlfriend\n",
      "battle\n",
      "setting\n",
      "baby\n",
      "incredibly\n",
      "earlier\n",
      "mess\n",
      "copy\n",
      "joe\n",
      "realize\n",
      "directors\n",
      "powerful\n",
      "rate\n",
      "dramatic\n",
      "joke\n",
      "pay\n",
      "following\n",
      "plenty\n",
      "directing\n",
      "dream\n",
      "ask\n",
      "development\n",
      "creepy\n",
      "appear\n",
      "brings\n",
      "front\n",
      "80\n",
      "rich\n",
      "admit\n",
      "political\n",
      "leading\n",
      "portrayed\n",
      "era\n",
      "gun\n",
      "cover\n",
      "present\n",
      "william\n",
      "party\n",
      "deserves\n",
      "fails\n",
      "30\n",
      "success\n",
      "break\n",
      "list\n",
      "meant\n",
      "zombie\n",
      "expecting\n",
      "create\n",
      "secret\n",
      "ideas\n",
      "ended\n",
      "manages\n",
      "agree\n",
      "missing\n",
      "villain\n",
      "match\n",
      "return\n",
      "caught\n",
      "cute\n",
      "unlike\n",
      "hardly\n",
      "german\n",
      "d\n",
      "spoilers\n",
      "clever\n",
      "laughing\n",
      "office\n",
      "members\n",
      "nudity\n",
      "plain\n",
      "italian\n",
      "escape\n",
      "public\n",
      "filmmakers\n",
      "century\n",
      "potential\n",
      "created\n",
      "wrote\n",
      "missed\n",
      "company\n",
      "married\n",
      "pure\n",
      "6\n",
      "scott\n",
      "filled\n",
      "further\n",
      "incredible\n",
      "cold\n",
      "sadly\n",
      "van\n",
      "visual\n",
      "familiar\n",
      "considering\n",
      "died\n",
      "store\n",
      "language\n",
      "speak\n",
      "popular\n",
      "sweet\n",
      "force\n",
      "tension\n",
      "convincing\n",
      "uses\n",
      "decides\n",
      "anti\n",
      "social\n",
      "entirely\n",
      "dancing\n",
      "bored\n",
      "band\n",
      "c\n",
      "neither\n",
      "intelligent\n",
      "15\n",
      "follows\n",
      "former\n",
      "law\n",
      "choice\n",
      "common\n",
      "appreciate\n",
      "brain\n",
      "college\n",
      "fit\n",
      "alive\n",
      "younger\n",
      "amount\n",
      "moves\n",
      "brothers\n",
      "violent\n",
      "depth\n",
      "biggest\n",
      "yeah\n",
      "concept\n",
      "cat\n",
      "90\n",
      "producers\n",
      "post\n",
      "project\n",
      "drug\n",
      "recommended\n",
      "successful\n",
      "exciting\n",
      "amusing\n",
      "pointless\n",
      "cause\n",
      "studio\n",
      "christmas\n",
      "spirit\n",
      "pathetic\n",
      "adventure\n",
      "alien\n",
      "meaning\n",
      "spend\n",
      "fake\n",
      "trash\n",
      "ben\n",
      "positive\n",
      "kills\n",
      "master\n",
      "decide\n",
      "respect\n",
      "consider\n",
      "questions\n",
      "vampire\n",
      "super\n",
      "garbage\n",
      "channel\n",
      "impressive\n",
      "changed\n",
      "shoot\n",
      "impossible\n",
      "humour\n",
      "rated\n",
      "walk\n",
      "involving\n",
      "blue\n",
      "f\n",
      "prison\n",
      "showed\n",
      "runs\n",
      "week\n",
      "shooting\n",
      "literally\n",
      "failed\n",
      "chemistry\n",
      "charles\n",
      "camp\n",
      "trip\n",
      "thanks\n",
      "touch\n",
      "tough\n",
      "south\n",
      "honestly\n",
      "west\n",
      "soldiers\n",
      "conclusion\n",
      "ability\n",
      "adult\n",
      "sam\n",
      "win\n",
      "jones\n",
      "mood\n",
      "mary\n",
      "added\n",
      "slasher\n",
      "surprisingly\n",
      "producer\n",
      "sexy\n",
      "tone\n",
      "bother\n",
      "chase\n",
      "normal\n",
      "issues\n",
      "bed\n",
      "edge\n",
      "personally\n",
      "heavy\n",
      "standard\n",
      "animated\n",
      "nowhere\n",
      "purpose\n",
      "door\n",
      "taste\n",
      "pieces\n",
      "beautifully\n",
      "likely\n",
      "everybody\n",
      "frank\n",
      "military\n",
      "london\n",
      "g\n",
      "naked\n",
      "aspect\n",
      "available\n",
      "appearance\n",
      "terms\n",
      "planet\n",
      "complex\n",
      "charlie\n",
      "catch\n",
      "photography\n",
      "bottom\n",
      "steve\n",
      "presence\n",
      "utterly\n",
      "ride\n",
      "date\n",
      "historical\n",
      "comedies\n",
      "ed\n",
      "adaptation\n",
      "chris\n",
      "victim\n",
      "managed\n",
      "appeal\n",
      "wow\n",
      "changes\n",
      "knowing\n",
      "o\n",
      "details\n",
      "plan\n",
      "thank\n",
      "track\n",
      "rare\n",
      "narrative\n",
      "smith\n",
      "attack\n",
      "loves\n",
      "unbelievable\n",
      "teen\n",
      "wild\n",
      "names\n",
      "besides\n",
      "minor\n",
      "intended\n",
      "painful\n",
      "harry\n",
      "student\n",
      "presented\n",
      "filming\n",
      "festival\n",
      "race\n",
      "finish\n",
      "equally\n",
      "brief\n",
      "hoping\n",
      "themes\n",
      "hurt\n",
      "70\n",
      "r\n",
      "paris\n",
      "mysterious\n",
      "critics\n",
      "sub\n",
      "surely\n",
      "manner\n",
      "aspects\n",
      "disappointment\n",
      "developed\n",
      "mainly\n",
      "worthy\n",
      "stunning\n",
      "double\n",
      "tired\n",
      "million\n",
      "hospital\n",
      "justice\n",
      "cry\n",
      "fascinating\n",
      "born\n",
      "stewart\n",
      "support\n",
      "opportunity\n",
      "martin\n",
      "lover\n",
      "holes\n",
      "fresh\n",
      "offer\n",
      "cliché\n",
      "dreams\n",
      "faces\n",
      "random\n",
      "news\n",
      "happening\n",
      "agent\n",
      "finding\n",
      "content\n",
      "billy\n",
      "jean\n",
      "folks\n",
      "exception\n",
      "drugs\n",
      "kelly\n",
      "summer\n",
      "batman\n",
      "beat\n",
      "likable\n",
      "dies\n",
      "pull\n",
      "ground\n",
      "fell\n",
      "gem\n",
      "cops\n",
      "seemingly\n",
      "lighting\n",
      "impressed\n",
      "radio\n",
      "actresses\n",
      "trailer\n",
      "collection\n",
      "plane\n",
      "machine\n",
      "approach\n",
      "industry\n",
      "soldier\n",
      "church\n",
      "count\n",
      "delivers\n",
      "anymore\n",
      "gorgeous\n",
      "christian\n",
      "wondering\n",
      "therefore\n",
      "detail\n",
      "element\n",
      "step\n",
      "led\n",
      "followed\n",
      "don\n",
      "serial\n",
      "40\n",
      "offers\n",
      "lets\n",
      "phone\n",
      "numbers\n",
      "animals\n",
      "cgi\n",
      "creature\n",
      "woods\n",
      "ready\n",
      "absolute\n",
      "negative\n",
      "becoming\n",
      "area\n",
      "thats\n",
      "jason\n",
      "onto\n",
      "fellow\n",
      "turning\n",
      "fox\n",
      "whatsoever\n",
      "tragic\n",
      "angry\n",
      "kevin\n",
      "torture\n",
      "cross\n",
      "helped\n",
      "accident\n",
      "ford\n",
      "member\n",
      "commentary\n",
      "listen\n",
      "artistic\n",
      "thoroughly\n",
      "wooden\n",
      "childhood\n",
      "moved\n",
      "process\n",
      "mouth\n",
      "horse\n",
      "plots\n",
      "x\n",
      "11\n",
      "12\n",
      "search\n",
      "sleep\n",
      "wars\n",
      "food\n",
      "judge\n",
      "sight\n",
      "dying\n",
      "rape\n",
      "wanting\n",
      "began\n",
      "memory\n",
      "sucks\n",
      "favourite\n",
      "tim\n",
      "religious\n",
      "rarely\n",
      "introduced\n",
      "loving\n",
      "hidden\n",
      "watchable\n",
      "chinese\n",
      "twenty\n",
      "tears\n",
      "artist\n",
      "intriguing\n",
      "accept\n",
      "al\n",
      "suspect\n",
      "allow\n",
      "brown\n",
      "comedic\n",
      "trust\n",
      "surprising\n",
      "soft\n",
      "mike\n",
      "wonderfully\n",
      "deserve\n",
      "calls\n",
      "accurate\n",
      "blame\n",
      "understanding\n",
      "mine\n",
      "reminds\n",
      "nicely\n",
      "criminal\n",
      "howard\n",
      "limited\n",
      "l\n",
      "board\n",
      "unusual\n",
      "pre\n",
      "apparent\n",
      "pacing\n",
      "hated\n",
      "gags\n",
      "wide\n",
      "driving\n",
      "vhs\n",
      "alan\n",
      "noticed\n",
      "whilst\n",
      "stock\n",
      "m\n",
      "w\n",
      "met\n",
      "unnecessary\n",
      "devil\n",
      "references\n",
      "academy\n",
      "creating\n",
      "learned\n",
      "pilot\n",
      "humans\n",
      "realism\n",
      "60\n",
      "according\n",
      "witch\n",
      "taylor\n",
      "mrs\n",
      "golden\n",
      "constant\n",
      "mission\n",
      "explained\n",
      "treated\n",
      "lewis\n",
      "magnificent\n",
      "window\n",
      "sean\n",
      "gary\n",
      "jump\n",
      "davis\n",
      "naturally\n",
      "national\n",
      "kick\n",
      "media\n",
      "cuts\n",
      "murdered\n",
      "prove\n",
      "included\n",
      "crappy\n",
      "jennifer\n",
      "villains\n",
      "instance\n",
      "european\n",
      "advice\n",
      "desire\n",
      "adam\n",
      "guns\n",
      "edited\n",
      "candy\n",
      "owner\n",
      "bomb\n",
      "max\n",
      "stereotypes\n",
      "discovered\n",
      "frankly\n",
      "lynch\n",
      "finale\n",
      "toward\n",
      "blind\n",
      "relate\n",
      "fame\n",
      "psychological\n",
      "favor\n",
      "anthony\n",
      "texas\n",
      "seat\n",
      "remembered\n",
      "african\n",
      "parody\n",
      "ann\n",
      "luck\n",
      "reaction\n",
      "screaming\n",
      "spectacular\n",
      "visuals\n",
      "unable\n",
      "satire\n",
      "gory\n",
      "evidence\n",
      "sheriff\n",
      "steal\n",
      "football\n",
      "decision\n",
      "trek\n",
      "creatures\n",
      "logic\n",
      "freedom\n",
      "curious\n",
      "perspective\n",
      "barbara\n",
      "contrived\n",
      "dan\n",
      "chick\n",
      "round\n",
      "europe\n",
      "alice\n",
      "hall\n",
      "graphic\n",
      "laughter\n",
      "jesus\n",
      "north\n",
      "creates\n",
      "combination\n",
      "underrated\n",
      "chaplin\n",
      "fred\n",
      "capable\n",
      "rules\n",
      "talks\n",
      "families\n",
      "contrast\n",
      "evening\n",
      "anna\n",
      "larry\n",
      "gene\n",
      "seagal\n",
      "louis\n",
      "prince\n",
      "treatment\n",
      "chan\n",
      "dogs\n",
      "jackie\n",
      "tedious\n",
      "portrays\n",
      "bbc\n",
      "angel\n",
      "sky\n",
      "chosen\n",
      "matters\n",
      "teens\n",
      "cage\n",
      "foot\n",
      "voices\n",
      "walks\n",
      "service\n",
      "weeks\n",
      "anderson\n",
      "twisted\n",
      "largely\n",
      "depressing\n",
      "werewolf\n",
      "hearing\n",
      "portraying\n",
      "saturday\n",
      "kung\n",
      "code\n",
      "dennis\n",
      "versions\n",
      "lane\n",
      "drop\n",
      "truck\n",
      "depicted\n",
      "segment\n",
      "fish\n",
      "appropriate\n",
      "jewish\n",
      "veteran\n",
      "latest\n",
      "guilty\n",
      "edward\n",
      "recognize\n",
      "murphy\n",
      "patient\n",
      "roger\n",
      "continuity\n",
      "speaks\n",
      "appealing\n",
      "irish\n",
      "conversation\n",
      "market\n",
      "grant\n",
      "parker\n",
      "sun\n",
      "suspenseful\n",
      "accents\n",
      "encounter\n",
      "deadly\n",
      "extraordinary\n",
      "forth\n",
      "claims\n",
      "victor\n",
      "nazi\n",
      "refreshing\n",
      "naive\n",
      "superman\n",
      "prepared\n",
      "destroyed\n",
      "serves\n",
      "sir\n",
      "eastwood\n",
      "mexican\n",
      "renting\n",
      "sides\n",
      "wolf\n",
      "summary\n",
      "daniel\n",
      "designed\n",
      "insight\n",
      "provoking\n",
      "quirky\n",
      "term\n",
      "status\n",
      "greater\n",
      "listening\n",
      "laura\n",
      "regarding\n",
      "theory\n",
      "expression\n",
      "timing\n",
      "sensitive\n",
      "indie\n",
      "baseball\n",
      "wwii\n",
      "anger\n",
      "tree\n",
      "dinner\n",
      "understood\n",
      "1970\n",
      "bound\n",
      "nuclear\n",
      "throws\n",
      "uninteresting\n",
      "mgm\n",
      "un\n",
      "fbi\n",
      "adding\n",
      "gross\n",
      "lloyd\n",
      "arm\n",
      "cheese\n",
      "minded\n",
      "vacation\n",
      "gruesome\n",
      "friendly\n",
      "chuck\n",
      "cowboy\n",
      "reference\n",
      "carried\n",
      "obnoxious\n",
      "blair\n",
      "drives\n",
      "entry\n",
      "shut\n",
      "jon\n",
      "novels\n",
      "happiness\n",
      "mentally\n",
      "seeking\n",
      "hip\n",
      "ron\n",
      "hardy\n",
      "union\n",
      "sinister\n",
      "2005\n",
      "sum\n",
      "sold\n",
      "fairy\n",
      "jay\n",
      "storm\n",
      "thief\n",
      "nose\n",
      "honor\n",
      "ad\n",
      "greek\n",
      "bergman\n",
      "thousand\n",
      "st\n",
      "torn\n",
      "flynn\n",
      "powell\n",
      "cases\n",
      "hint\n",
      "marvelous\n",
      "useless\n",
      "connery\n",
      "stylish\n",
      "ninja\n",
      "tribute\n",
      "smoking\n",
      "irony\n",
      "object\n",
      "intentions\n",
      "hooked\n",
      "snakes\n",
      "meaningful\n",
      "dawn\n",
      "sandra\n",
      "suits\n",
      "size\n",
      "outer\n",
      "sidney\n",
      "lacked\n",
      "piano\n",
      "competition\n",
      "beings\n",
      "guilt\n",
      "holly\n",
      "ralph\n",
      "stretch\n",
      "identify\n",
      "discovery\n",
      "laurel\n",
      "jake\n",
      "bone\n",
      "hal\n",
      "carter\n",
      "hoped\n",
      "cinderella\n",
      "editor\n",
      "precious\n",
      "neighborhood\n",
      "kudos\n",
      "gain\n",
      "commit\n",
      "fitting\n",
      "bruno\n",
      "altogether\n",
      "shirt\n",
      "claire\n",
      "heat\n",
      "allowing\n",
      "glenn\n",
      "superbly\n",
      "jeremy\n",
      "mild\n",
      "celluloid\n",
      "lab\n",
      "displays\n",
      "terrorist\n",
      "fears\n",
      "prevent\n",
      "lou\n",
      "brando\n",
      "burned\n",
      "complaint\n",
      "staged\n",
      "roman\n",
      "sole\n",
      "juvenile\n",
      "campbell\n",
      "conversations\n",
      "mario\n",
      "empire\n",
      "influenced\n",
      "groups\n",
      "philosophy\n",
      "terry\n",
      "harder\n",
      "martha\n",
      "sidekick\n",
      "holy\n",
      "rendition\n",
      "iron\n",
      "boll\n",
      "clothing\n",
      "grab\n",
      "nicole\n",
      "bourne\n",
      "selfish\n",
      "armed\n",
      "enemies\n",
      "education\n",
      "loads\n",
      "lay\n",
      "progress\n",
      "troma\n",
      "barney\n",
      "cassavetes\n",
      "advise\n",
      "passionate\n",
      "streisand\n",
      "convincingly\n",
      "pretending\n",
      "mail\n",
      "cream\n",
      "mankind\n",
      "jeffrey\n",
      "mayor\n",
      "bay\n",
      "yelling\n",
      "portion\n",
      "bullock\n",
      "apes\n",
      "illness\n",
      "banned\n",
      "spain\n",
      "spider\n",
      "crawford\n",
      "hung\n",
      "literature\n",
      "’\n",
      "sue\n",
      "ape\n",
      "creator\n",
      "serving\n",
      "pal\n",
      "june\n",
      "refused\n",
      "resist\n",
      "kitty\n",
      "jordan\n",
      "judy\n",
      "disappears\n",
      "bettie\n",
      "stale\n",
      "objects\n",
      "advanced\n",
      "improvement\n",
      "erika\n",
      "occasions\n",
      "uwe\n",
      "dirt\n",
      "females\n",
      "dickens\n",
      "phillips\n",
      "distribution\n",
      "macbeth\n",
      "route\n",
      "andre\n",
      "obligatory\n",
      "simpsons\n",
      "snipes\n",
      "kramer\n",
      "teams\n",
      "travolta\n",
      "herzog\n",
      "regards\n",
      "mitchum\n",
      "gloria\n",
      "dillon\n",
      "bacon\n",
      "debbie\n",
      "critters\n",
      "marked\n",
      "showtime\n",
      "filth\n",
      "altered\n",
      "apocalyptic\n",
      "youtube\n",
      "polanski\n",
      "imaginary\n",
      "mins\n",
      "tapes\n",
      "incidents\n",
      "costello\n",
      "asia\n",
      "tossed\n",
      "sensible\n",
      "flag\n",
      "corey\n",
      "gorilla\n",
      "mencia\n",
      "hk\n",
      "ants\n",
      "reagan\n",
      "pam\n",
      "ny\n",
      "glen\n",
      "moses\n",
      "chucky\n",
      "nathan\n",
      "ghoulies\n",
      "casper\n",
      "pizza\n",
      "tolerance\n",
      "fuel\n",
      "norton\n",
      "grayson\n",
      "alicia\n",
      "barbra\n",
      "daffy\n",
      "leland\n",
      "norma\n",
      "nanny\n",
      "brett\n",
      "peggy\n",
      "rousing\n",
      "antwone\n",
      "phoebe\n",
      "gein\n",
      "amir\n",
      "oblivious\n",
      "thirst\n",
      "shuttle\n",
      "zoey\n",
      "asterix\n",
      "shylock\n",
      "lestat\n",
      "marlow\n",
      "developments\n",
      "eleanor\n",
      "letterman\n",
      "sylvester\n",
      "quigley\n",
      "chamberlain\n",
      "37\n",
      "sheba\n",
      "estevez\n",
      "episodic\n",
      "crossfire\n",
      "dev\n",
      "hawaii\n",
      "sorority\n",
      "kieslowski\n",
      "deformed\n",
      "lifeforce\n",
      "anand\n",
      "symphony\n",
      "rififi\n",
      "yadda\n",
      "suchet\n",
      "bardem\n",
      "linnea\n",
      "formal\n",
      "tipping\n",
      "mona\n",
      "riddick\n",
      "basterds\n",
      "morocco\n",
      "finn\n",
      "candles\n",
      "bess\n",
      "peebles\n",
      "ozu\n",
      "tonto\n",
      "tamura\n",
      "manuel\n",
      "chipmunks\n",
      "ja\n",
      "zeppelin\n",
      "sherri\n",
      "kasparov\n",
      "aeon\n",
      "sera\n",
      "comanche\n",
      "goya\n",
      "mclaren\n",
      "digges\n",
      "octopussy\n",
      "danson\n",
      "partisans\n",
      "larson\n",
      "aaliyah\n",
      "lackawanna\n",
      "eko\n",
      "owls\n",
      "arbus\n",
      "caddy\n",
      "hemmings\n",
      "alyssa\n",
      "underdogs\n",
      "davidians\n",
      "shayne\n",
      "skateboarder\n",
      "huntz\n",
      "frewer\n",
      "dissecting\n",
      "potyomkin\n",
      "moores\n",
      "sajani\n",
      "caw\n",
      "maura\n",
      "tallest\n",
      "gyrations\n",
      "egger\n",
      "handlebar\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reversed_frequent_unigrams = dict([(value, key) for (key, value) in frequent_unigrams.items()])\n",
    "reversed_frequent_unigrams_top_values = sorted(reversed_frequent_unigrams.keys(), reverse=True)[:]\n",
    "#print(reversed_frequent_unigrams_top_values)\n",
    "\n",
    "# print unigrams top values\n",
    "for x in reversed_frequent_unigrams_top_values:\n",
    "    print(reversed_frequent_unigrams[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('br', 'br')\n",
      "('of', 'the')\n",
      "('in', 'the')\n",
      "('it', \"'s\")\n",
      "('this', 'movie')\n",
      "('the', 'film')\n",
      "('and', 'the')\n",
      "('is', 'a')\n",
      "('the', 'movie')\n",
      "('to', 'the')\n",
      "('to', 'be')\n",
      "('this', 'film')\n",
      "('it', 'is')\n",
      "('this', 'is')\n",
      "('it', 'was')\n",
      "('on', 'the')\n",
      "('in', 'a')\n",
      "('do', \"n't\")\n",
      "('one', 'of')\n",
      "('for', 'the')\n"
     ]
    }
   ],
   "source": [
    "# Extract top bigrams\n",
    "extracted_bigrams = extract_bigrams(decoded_sentences);\n",
    "\n",
    "# Get most frequent bigrams into a dictionary\n",
    "frequent_bigrams = dict()\n",
    "for x in extracted_bigrams:\n",
    "    if x in frequent_bigrams:\n",
    "            frequent_bigrams[x] += 1\n",
    "    else:\n",
    "        frequent_bigrams[x] = 1\n",
    "        \n",
    "    \n",
    "reversed_frequent_bigrams = dict([(value, key) for (key, value) in frequent_bigrams.items()])\n",
    "reversed_frequent_bigrams_top_values = sorted(reversed_frequent_bigrams.keys(), reverse=True)[:20]\n",
    "# print(reversed_frequent_bigrams_top_values)\n",
    "\n",
    "# print unigrams top values\n",
    "for x in reversed_frequent_bigrams_top_values:\n",
    "    print(reversed_frequent_bigrams[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting BOW\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "#CountVectorizer implements both tokenization and occurrence counting in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences_as_bag_of_words = vectorizer.fit_transform(decoded_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectors = sentences_as_bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = pd.DataFrame(targets)\n",
    "dataset_frame = pd.DataFrame(data=bow_vectors)\n",
    "dataset_frame['Class'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select lessa data\n",
    "selected_number = 2001\n",
    "dataset_reduced = dataset_frame[:selected_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "#Split the data into a training and a testing set\n",
    "bound_param = 1930\n",
    "train_features = dataset_reduced.iloc[:bound_param,:-1] \n",
    "test_features = dataset_reduced.iloc[bound_param:,:-1] \n",
    "train_labels = dataset_reduced.iloc[:bound_param,-1] \n",
    "test_labels = dataset_reduced.iloc[bound_param:,-1]\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy').fit(train_features,train_labels)#The accuracy is then calculated through the score function offered by sklearn\n",
    "print(\"The prediction accuracy is: \",tree.score(test_features,test_labels)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test labels to array to iterate through\n",
    "labels_dataFrame = pd.DataFrame(test_labels.values)\n",
    "labels_dataFrame = labels_dataFrame.to_numpy()\n",
    "\n",
    "# make predictions\n",
    "prediction = tree.predict(test_features)\n",
    "prediction\n",
    "\n",
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "for count,x in enumerate(prediction):\n",
    "    if labels_dataFrame[count] == x:\n",
    "        print(f'Correct, {x}')\n",
    "        correct_count+=1\n",
    "    else:\n",
    "        print(f'Wrong, {x}')\n",
    "        wrong_count+=1\n",
    "        \n",
    "\n",
    "#print(f'Correctly predicted: {correct_count} \\nWrongly predicted: {wrong_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = io.StringIO()\n",
    "export_graphviz(tree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# f1 score best value at 1 and worse at 0\n",
    "from sklearn.metrics import f1_score\n",
    "y_true = labels_dataFrame\n",
    "y_pred = prediction\n",
    "f1_score(y_true, y_pred, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
