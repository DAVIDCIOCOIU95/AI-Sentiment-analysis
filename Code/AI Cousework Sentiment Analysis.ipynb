{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set\n",
    "from keras.datasets import imdb\n",
    "# split the data set into training and testing target-data\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the words already tokenized(remember that the words are unique, each having an ID) -> returns a dictionary\n",
    "index = imdb.get_word_index()\n",
    "# reverse key-value pair in dictionary\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# decode a word (this is sample code to be used later)\n",
    "decoded = \" \".join([reverse_index.get(i -3, \"\") for i in training_data[0]] )\n",
    "print(decoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 20 unigrams in the data set\n",
    "# Steps:\n",
    "# 1 decode all the sentences\n",
    "# 2 tokenize\n",
    "# 3 count how many times each word recurs\n",
    "# 4 order the result and print the words\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: decode all the sentences\n",
    "# extract all the sentences from training data and test data decoded\n",
    "decoded_sentences = []\n",
    "# extract from training data\n",
    "for x in range(len(training_data)):\n",
    "    decoded_sentences.append(\" \".join([reverse_index.get(i - 3, \"\") for i in training_data[x]]))\n",
    "\n",
    "# extract from testing data\n",
    "for x in range(len(testing_data)):\n",
    "    decoded_sentences.append(\" \".join([reverse_index.get(i - 3, \"\") for i in testing_data[x]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# The function will split everything in Unigrams\n",
    "def extract_unigram(sentences): \n",
    "\n",
    "  tokens = []\n",
    "  for sentence in sentences:\n",
    "    tok = word_tokenize(sentence)\n",
    "    for t in tok:\n",
    "      tokens.append(t)\n",
    "  return tokens\n",
    "\n",
    "\n",
    "\n",
    "# The function will split everything in Bigrams\n",
    "def extract_bigrams(sentences): \n",
    "\n",
    "  all_bigrams = []\n",
    "  for sentence in sentences:\n",
    "    token = word_tokenize(sentence)\n",
    "    bigrams = ngrams(token,2)\n",
    "    for b in bigrams:\n",
    "      all_bigrams.append(b)\n",
    "  return all_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "is\n",
      "br\n",
      "it\n",
      "in\n",
      "i\n",
      "this\n",
      "that\n",
      "'s\n",
      "was\n",
      "as\n",
      "movie\n",
      "for\n",
      "with\n",
      "but\n",
      "film\n"
     ]
    }
   ],
   "source": [
    "# Extract top unigrams\n",
    "extracted_unigrams = extract_unigram(decoded_sentences);\n",
    "\n",
    "# Get most frequent unigrams into a dictionary\n",
    "frequent_unigrams = dict()\n",
    "for x in extracted_unigrams:\n",
    "    if x in frequent_unigrams:\n",
    "            frequent_unigrams[x] += 1\n",
    "    else:\n",
    "        frequent_unigrams[x] = 1\n",
    "\n",
    "\n",
    "\n",
    "reversed_frequent_unigrams = dict([(value, key) for (key, value) in frequent_unigrams.items()])\n",
    "reversed_frequent_unigrams_top_values = sorted(reversed_frequent_unigrams.keys(), reverse=True)[:20]\n",
    "#print(reversed_frequent_unigrams_top_values)\n",
    "\n",
    "# print unigrams top values\n",
    "for x in reversed_frequent_unigrams_top_values:\n",
    "    print(reversed_frequent_unigrams[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('br', 'br')\n",
      "('of', 'the')\n",
      "('in', 'the')\n",
      "('it', \"'s\")\n",
      "('this', 'movie')\n",
      "('the', 'film')\n",
      "('and', 'the')\n",
      "('is', 'a')\n",
      "('the', 'movie')\n",
      "('to', 'the')\n",
      "('to', 'be')\n",
      "('this', 'film')\n",
      "('it', 'is')\n",
      "('this', 'is')\n",
      "('it', 'was')\n",
      "('on', 'the')\n",
      "('in', 'a')\n",
      "('do', \"n't\")\n",
      "('one', 'of')\n",
      "('for', 'the')\n"
     ]
    }
   ],
   "source": [
    "# Extract top bigrams\n",
    "extracted_bigrams = extract_bigrams(decoded_sentences);\n",
    "\n",
    "# Get most frequent bigrams into a dictionary\n",
    "frequent_bigrams = dict()\n",
    "for x in extracted_bigrams:\n",
    "    if x in frequent_bigrams:\n",
    "            frequent_bigrams[x] += 1\n",
    "    else:\n",
    "        frequent_bigrams[x] = 1\n",
    "        \n",
    "    \n",
    "reversed_frequent_bigrams = dict([(value, key) for (key, value) in frequent_bigrams.items()])\n",
    "reversed_frequent_bigrams_top_values = sorted(reversed_frequent_bigrams.keys(), reverse=True)[:20]\n",
    "# print(reversed_frequent_bigrams_top_values)\n",
    "\n",
    "# print unigrams top values\n",
    "for x in reversed_frequent_bigrams_top_values:\n",
    "    print(reversed_frequent_bigrams[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
