{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set\n",
    "from keras.datasets import imdb\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# split the data set into training and testing target-data\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data()\n",
    "\n",
    "# get all the words already tokenized(remember that the words are unique, each having an ID) -> returns a dictionary\n",
    "index = imdb.get_word_index()\n",
    "# reverse key-value pair in dictionary\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "\n",
    "\n",
    "# STEP 1: decode all the sentences\n",
    "# extract all the sentences from training data and test data decoded\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "decoded_sentences = []\n",
    "# extract from training data\n",
    "for x in range(len(data)):\n",
    "    decoded_sentences.append(\" \".join([reverse_index.get(i - 3, \"#\") for i in data[x]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in range(0,5000):\n",
    "    #decoded = \" \".join([reverse_index.get(i -3, \"#\") for i in training_data[x]])\n",
    "    #print(decoded) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "is\n",
      "br\n",
      "in\n",
      "it\n",
      "i\n",
      "this\n",
      "that\n",
      "was\n",
      "as\n",
      "for\n",
      "with\n",
      "movie\n",
      "but\n",
      "film\n",
      "on\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,21):\n",
    "    print(reverse_index.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(decoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(decoded_sentences)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(decoded_sentences[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del training_data\n",
    "#del training_targets\n",
    "#del testing_data\n",
    "#del testing_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract less data (memory issues)\n",
    "# take less data as there isn't enough memory\n",
    "#dataset_frame_reduced = dataset_frame.iloc[:5000,:]\n",
    "\n",
    "# get only the sentences without the label, then flatten array so you can pass it to countvectorizer\n",
    "#dataset_reduced_sentences = dataset_frame_reduced.iloc[:,:-1].values.flatten()\n",
    "\n",
    "#stopwords = nltk.corpus.stopwords.words('english')\n",
    "#newStopWords = ['\\x96','\\x86']\n",
    "#stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize all\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=9, max_df=0.5, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the labels back\n",
    "#dataset_vectorized = pd.DataFrame(data=X)\n",
    "#dataset_vectorized['Class'] = dataset_frame_reduced.iloc[:,(len(dataset_frame_reduced.columns) -1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data frame and appending labels so to extract a normal distribution\n",
    "labels = pd.DataFrame(targets)\n",
    "dataset_frame = pd.DataFrame(data=X)\n",
    "dataset_frame['Class'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_frame_reduced = dataset_frame.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 1000\n",
    "reviews_negative = dataset_frame_reduced[dataset_frame_reduced['Class'] == 0].iloc[:bound,:]\n",
    "reviews_positive = dataset_frame_reduced[dataset_frame_reduced['Class'] == 1].iloc[:bound,:]\n",
    "#extract a normal distribution according to an upper bound\n",
    "reviews = reviews_negative.append(reviews_positive, ignore_index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews.iloc[:,:-1],reviews.iloc[:,(len(reviews.columns) -1):], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=13,criterion = 'entropy').fit(X_train,y_train)#The accuracy is then calculated through the score function offered by sklearn\n",
    "print(\"The prediction accuracy is: \",tree.score(X_test,y_test)*100,\"%\")\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix\", None)]\n",
    "class_names = ['Negative', 'Positive']\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(tree, X_test, y_test,\n",
    "                                    display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classes\n",
    "print(\"Training set:\\n\",\"0\", y_train.loc[y_train['Class'] == 0].count().to_string(), \"\\n1\", y_train.loc[y_train['Class'] == 1].count().to_string())\n",
    "\n",
    "print(\"Testing set:\\n\",\"0\", y_test.loc[y_test['Class'] == 0].count().to_string(), \"\\n1\", y_test.loc[y_test['Class'] == 1].count().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-folds evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with k folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = DecisionTreeClassifier(max_depth=13,criterion = 'entropy', random_state=0)\n",
    "k_folds = cross_val_score(clf, reviews.iloc[:,:-1], reviews.iloc[:,(len(reviews.columns) -1):], cv=10)\n",
    "\n",
    "# Python program to get average of a list \n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "  \n",
    "# Driver Code \n",
    "average = Average(k_folds) \n",
    "  \n",
    "# Printing average of the list \n",
    "print(\"Average of the list =\", round(average, 2) * 100 , \"%\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.distplot(k_folds)\n",
    "plt.title('Average score: {}'.format(np.mean(k_folds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = io.StringIO()\n",
    "export_graphviz(tree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[410])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
